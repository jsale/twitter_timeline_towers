{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get User Timeline Tweets\n",
    "## This script takes an array of usernames and queries Twitter for their most recent 3200 tweets and saves them to JSON files, 100 tweets per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2022-12-14\n",
    "\n",
    "@author: jsale\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example lists containing twitter users\n",
    "test = ['CBSMornings','nytimes','TechCrunch']\n",
    "# Set the twitter_user_array equal to one of the example lists\n",
    "twitter_user_array = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory for each user's tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Optional Zip folder creation\n",
    "\n",
    "cwd_path = os.getcwd()\n",
    "\n",
    "# Save to a 'timelines' subfolder\n",
    "for i in range(len(twitter_user_array)):\n",
    "    new_dir = cwd_path + \"/timelines/\" + twitter_user_array[i]\n",
    "    try:\n",
    "        os.mkdir(new_dir)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % new_dir)\n",
    "    else:\n",
    "        print (\"Directory %s successfully created.\" % new_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get user timeline tweets and save as JSON with 100 tweets per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd_path = os.getcwd()\n",
    "\n",
    "# Enter your Twitter API keys here\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "access_token=\"\"\n",
    "access_token_secret=\"\"\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    r.headers[\"Authorization\"] = \"Bearer <<Your Bearer Token Here>>\"\n",
    "    return r\n",
    "\n",
    "outdir = cwd_path + '/timelines/'\n",
    "follower_tweet_count = []\n",
    "ftc_inc = 0\n",
    "for username in twitter_user_array:\n",
    "    # Optional print statement for tracking progress\n",
    "    print(\"Working on \" + username)\n",
    "    \n",
    "    # Get user_id from username because user_id is needed to query user timeline tweets\n",
    "    user_fields = \"user.fields=id,description,created_at\"\n",
    "    url = \"https://api.twitter.com/2/users/by?usernames=\" + username + \"&user.fields=id,description,created_at\"\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "    # Optional print statement for tracking progress\n",
    "    print(\"user_id successful \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    json_response = response.json()\n",
    "    user_id = json_response['data'][0]['id']\n",
    "    \n",
    "    # Get first page of user timeline tweets to initialize next_token field\n",
    "    url = \"https://api.twitter.com/2/users/\" + str(user_id) + \"/tweets\"\n",
    "\n",
    "#     params = {\"tweet.fields\": \"id,text,attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,in_reply_to_user_id,lang,possibly_sensitive,referenced_tweets,public_metrics,reply_settings,source,withheld&user.fields=id,name,profile_image_url,url,username&expansions=referenced_tweets.id,referenced_tweets.id.author_id,entities.mentions.username,in_reply_to_user_id,attachments.media_keys&media.fields=preview_image_url,type,url\",\"max_results\":100 }\n",
    "    params = {\"tweet.fields\": \"id,text,attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,in_reply_to_user_id,lang,possibly_sensitive,referenced_tweets,public_metrics,reply_settings,source,withheld\",\"max_results\":100 }\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
    "    print(\"0 \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    json_response =  response.json()\n",
    "    with open(cwd_path + outdir + username + '/' + username + '00.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "        json.dump(json_response, outfile) \n",
    "        \n",
    "    # Loop through up to 33 pages of user timeline tweets because of 3200 tweet query limit\n",
    "    for i in range(1,33):\n",
    "        if 'meta' in json_response:\n",
    "            if 'next_token' in json_response['meta']:\n",
    "    #             params = {\"tweet.fields\": \"id,text,attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,in_reply_to_user_id,lang,possibly_sensitive,referenced_tweets,public_metrics,reply_settings,source,withheld&user.fields=id,name,profile_image_url,url,username&expansions=referenced_tweets.id,referenced_tweets.id.author_id,entities.mentions.username,in_reply_to_user_id,attachments.media_keys&media.fields=preview_image_url,type,url\",\"max_results\":100,\"pagination_token\":json_response['meta']['next_token']  }\n",
    "                params = {\"tweet.fields\": \"id,text,attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,in_reply_to_user_id,lang,possibly_sensitive,referenced_tweets,public_metrics,reply_settings,source,withheld\",\"max_results\":100,\"pagination_token\":json_response['meta']['next_token']  }\n",
    "                response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
    "                print(str(i) + \" \" + str(response.status_code))\n",
    "                if response.status_code != 200:\n",
    "                    raise Exception(\n",
    "                        \"Request returned an error: {} {}\".format(\n",
    "                            response.status_code, response.text\n",
    "                        )\n",
    "                    )\n",
    "                json_response =  response.json()\n",
    "\n",
    "                # If i is less than 10, add a zero before the i in the filename\n",
    "                if i < 10:\n",
    "                    with open(cwd_path + outdir + username + '/' + username + \"0\" + str(i) + '.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "                        json.dump(json_response, outfile)        \n",
    "                else:\n",
    "                    with open(cwd_path + outdir + username + '/' + username + str(i) + '.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "                        json.dump(json_response, outfile)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files into array of tweets and run utilities to check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "cwd_path = os.getcwd()\n",
    "# for bot_inc in range(len(twitter_user_array)):\n",
    "for bot_inc in range(3):\n",
    "    pprint(\"bot_inc:\" + str(bot_inc) + \" | Working on user \" + twitter_user_array[bot_inc])\n",
    "\n",
    "    # Define variables\n",
    "    all_tweets = []\n",
    "    inc = 0\n",
    "    val = 0\n",
    "    val_inc = 0\n",
    "    dir = cwd_path + \"/timelines/myfollowers_tweets/\" + twitter_user_array[bot_inc] + '/'\n",
    "    filenames = next(os.walk(dir))[2]\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        with open(dir + filename, 'r', encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            for tweet in data['data']:\n",
    "#                 tweet = json.loads(line)\n",
    "                all_tweets.append(tweet)\n",
    "\n",
    "                # Increment variables to track progress, mostly for very large files\n",
    "                inc += 1\n",
    "                val_inc += 1\n",
    "                if val_inc > 100:\n",
    "                    val = val + 100\n",
    "                    print(str(val))\n",
    "                    val_inc = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_tweets)):\n",
    "    print(str(i) + \" \" + all_tweets[i]['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in all_tweets:\n",
    "#     print(tweet['id'])\n",
    "    if \"entities\" in tweet:\n",
    "        if \"urls\" in  tweet['entities']:\n",
    "            url_size = len(tweet['entities']['urls'])\n",
    "            print(str(url_size))\n",
    "        else:\n",
    "            print(\"no urls\")\n",
    "    else:\n",
    "        print('no entities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = 0\n",
    "for tweet in all_tweets:\n",
    "#     print(tweet['id'])\n",
    "    if \"referenced_tweets\" in tweet:\n",
    "        print(str(inc))\n",
    "        print(tweet['referenced_tweets'][0]['type'])\n",
    "    else:\n",
    "        print(str(inc) + ' no referenced_tweets')\n",
    "    inc = inc + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = 0\n",
    "for tweet in all_tweets:\n",
    "#     print(tweet['id'])\n",
    "    if \"entities\" in tweet:\n",
    "        if \"hashtags\" in tweet['entities']:\n",
    "            print(str(inc))\n",
    "            print(\"# of hashtags: \" + str(len(tweet['entities']['hashtags'])))\n",
    "            print(tweet['entities']['hashtags'])\n",
    "        else:\n",
    "            print(\"no hashtags\")\n",
    "    else:\n",
    "        print(str(inc) + ' no entities')\n",
    "    inc = inc + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_tweets[0]['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
